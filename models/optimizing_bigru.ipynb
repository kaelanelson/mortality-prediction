{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "import random\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_month(df):\n",
    "    unique_zips = np.unique(df_train.zip)\n",
    "\n",
    "    df_ls = []\n",
    "    for z in unique_zips:\n",
    "        df_sub = df[df.zip == z]\n",
    "        df_sub['month_continuous'] = list(range(1,df_sub.shape[0]+1))\n",
    "        df_ls.append(df_sub)\n",
    "    return pd.concat(df_ls)\n",
    "\n",
    "def agg_monthly(col_to_agg, df_train, df_test, groupby_cols):\n",
    "    # connvert month\n",
    "    df_train_v2 = convert_month(df_train)\n",
    "    df_test_v2 = convert_month(df_test)\n",
    "    \n",
    "    # aggregate monthly\n",
    "    df_train_agg = df_train_v2.groupby(groupby_cols).aggregate(col_to_agg).reset_index()\n",
    "    df_test_agg = df_test_v2.groupby(groupby_cols).aggregate(col_to_agg).reset_index()\n",
    "\n",
    "    return df_train_agg, df_test_agg\n",
    "\n",
    "def split_sequence3(df, x_cols, y_col, n_steps):\n",
    "    X,y = [],[] \n",
    "    zips = np.unique(df.zip)\n",
    "    \n",
    "    # loop through all zips\n",
    "    for i in range(zips.shape[0]): \n",
    "        \n",
    "        X_sub, y_sub = [],[]\n",
    "        \n",
    "        # subset to this zipcode\n",
    "        df_zip = df[df.zip == zips[i]]\n",
    "        full_dim = df_zip.shape[0]\n",
    "        \n",
    "        for j in range(full_dim):\n",
    "            # find the end of this pattern\n",
    "            end_jx = j + n_steps\n",
    "            out_end_ix = end_jx + n_steps\n",
    "\n",
    "            # check if we are beyond the sequence\n",
    "            if out_end_ix > full_dim:\n",
    "                break\n",
    "\n",
    "            # else, gather input and output parts of the pattern\n",
    "            x_sub_seq = df_zip[x_cols].iloc[j:end_jx]\n",
    "            y_sub_seq = df_zip[y_col].iloc[end_jx:out_end_ix]\n",
    "\n",
    "            X_sub.append(x_sub_seq.values)\n",
    "            y.append(y_sub_seq.values)\n",
    "\n",
    "        X.append(X_sub)\n",
    "    \n",
    "    #  return next index\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def normalize_data(X_train, X_test=None):\n",
    "    scaler = MinMaxScaler()\n",
    "    if X_test == None:\n",
    "        X_train_normalized = scaler.fit_transform(X_train)\n",
    "        return X_train_normalized\n",
    "    else:\n",
    "        X_train_normalized = scaler.fit_transform(X_train)\n",
    "        X_test_normalized = scaler.transform(X_test)\n",
    "        return X_train_normalized,X_test_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_agg = {'sex':'mean', 'age':'mean','deaths':sum, # 'death':sum, 'dead':'mean', \n",
    "       'dual':sum, 'poverty': 'mean', 'popdensity': 'mean', \n",
    "       'medianhousevalue': 'mean','pct_blk': 'mean', 'medhouseholdincome' : 'mean', 'pct_owner_occ': 'mean',\n",
    "       'hispanic': 'mean','education': 'mean', 'smoke_rate': 'mean', 'mean_bmi': 'mean', \n",
    "       'rmax': 'mean', 'pr': 'mean', 'population':sum,\n",
    "       'race_0':'mean', 'race_1':'mean', 'race_2':'mean', 'race_3':'mean', \n",
    "       'race_4':'mean', 'race_5':'mean', 'race_6':'mean',\n",
    "       'ICU_DAY':sum, 'CCI_DAY':sum, 'LOS':'mean', 'Parkinson_pdx2dx_25':sum,\n",
    "       'Alzheimer_pdx2dx_25':sum, 'Dementia_pdx2dx_25':sum, 'CHF_pdx2dx_25':sum,\n",
    "       'AMI_pdx2dx_25':sum, 'COPD_pdx2dx_25':sum, 'DM_pdx2dx_25':sum, 'Stroke_pdx2dx_25':sum,\n",
    "       'CVD_pdx2dx_25':sum, 'CSD_pdx2dx_25':sum, 'Ischemic_stroke_pdx2dx_25':sum,\n",
    "       'Hemo_Stroke_pdx2dx_25':sum, 'neo_140_149':sum, 'neo_150_159':sum, 'neo_160_165':sum,\n",
    "       'neo_170_176':sum, 'neo_179_189':sum, 'neo_190_199':sum, 'neo_200_209':sum,\n",
    "       'neo_210_229':sum, 'neo_230_234':sum, 'neo_235_238':sum, 'neo_239':sum, \n",
    "       'pm25_summer_4y_avg':'mean', 'pm25_winter_4y_avg':'mean', 'pm25_fall_4y_avg':'mean', 'pm25_spring_4y_avg': 'mean',\n",
    "       'ozone_summer_4y_avg':'mean', 'ozone_winter_4y_avg':'mean', 'ozone_fall_4y_avg': 'mean', 'ozone_spring_4y_avg': 'mean',\n",
    "       'no2_summer_4y_avg':'mean', 'no2_winter_4y_avg': 'mean', 'no2_fall_4y_avg': 'mean', 'no2_spring_4y_avg': 'mean', \n",
    "       'summer_tmmx_4y_avg': 'mean','summer_rmax_4y_avg': 'mean', \n",
    "       'winter_tmmx_4y_avg': 'mean', 'winter_rmax_4y_avg': 'mean',\n",
    "        'm_count':sum, 'f_count':sum, 'mean_age':'mean',\n",
    "       'white_count':sum, 'black_count':sum, 'hispanic_count':sum, 'asian_count':sum,\n",
    "       'native_count':sum, 'monthly_pop':'mean' #,'deaths_next_year': sum\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col2 = ['sex','age','poverty', 'popdensity', 'medianhousevalue',\n",
    "       'medhouseholdincome', 'pct_owner_occ', 'education',\n",
    "       'smoke_rate', 'mean_bmi', 'rmax', 'pr', 'ICU_DAY',\n",
    "       'CCI_DAY', 'LOS', 'Parkinson_pdx2dx_25', 'Alzheimer_pdx2dx_25',\n",
    "       'Dementia_pdx2dx_25', 'CHF_pdx2dx_25', 'AMI_pdx2dx_25',\n",
    "       'COPD_pdx2dx_25', 'DM_pdx2dx_25', 'Stroke_pdx2dx_25', 'CVD_pdx2dx_25',\n",
    "       'CSD_pdx2dx_25', 'Ischemic_stroke_pdx2dx_25', 'Hemo_Stroke_pdx2dx_25',\n",
    "       'neo_140_149', 'neo_150_159', 'neo_160_165', 'neo_170_176',\n",
    "       'neo_179_189', 'neo_190_199', 'neo_200_209', 'neo_210_229',\n",
    "       'neo_230_234', 'neo_235_238', 'neo_239', 'pm25_summer_4y_avg',\n",
    "       'pm25_winter_4y_avg', 'pm25_fall_4y_avg', 'pm25_spring_4y_avg',\n",
    "       'ozone_summer_4y_avg', 'ozone_winter_4y_avg', 'ozone_fall_4y_avg',\n",
    "       'ozone_spring_4y_avg', 'no2_summer_4y_avg', 'no2_winter_4y_avg',\n",
    "       'no2_fall_4y_avg', 'no2_spring_4y_avg', 'summer_tmmx_4y_avg',\n",
    "       'summer_rmax_4y_avg', 'winter_tmmx_4y_avg', 'winter_rmax_4y_avg',\n",
    "       'm_count', 'f_count', 'mean_age', 'white_count', 'black_count',\n",
    "       'hispanic_count', 'asian_count', 'native_count', 'monthly_pop']\n",
    "\n",
    "y_col1 = ['deaths']\n",
    "\n",
    "df_train = pd.read_csv('zip_train_v3.csv').drop(columns=['Unnamed: 1','zip.1'])\n",
    "df_test = pd.read_csv('zip_test_v3.csv').drop(columns=['Unnamed: 1','zip.1'])\n",
    "\n",
    "groupby_cols1 = ['zip','AYEAR','month_continuous']\n",
    "df_train_agg2, df_test_agg2 = agg_monthly(col_to_agg, df_train, df_test, groupby_cols1)\n",
    "\n",
    "n_steps=12\n",
    "\n",
    "# problem: test data - does not have y value for 12 months/1 year in advance\n",
    "X_train_rnn3, y_train_rnn3 = split_sequence3(df_train_agg2, x_col2, y_col1, n_steps)\n",
    "#X_test_rnn3, y_test_rnn3 = split_sequence3(test_temp, x_col2, y_col1, n_steps)\n",
    "\n",
    "# reshape to 2D for normalizing\n",
    "xdim2 = len(x_col2)\n",
    "\n",
    "X_train_rnn3_re = X_train_rnn3.reshape((X_train_rnn3.shape[0]*X_train_rnn3.shape[1]*n_steps,xdim2))\n",
    "y_train_rnn3_re = y_train_rnn3.reshape((y_train_rnn3.shape[0],y_train_rnn3.shape[1]))\n",
    "\n",
    "# normalize\n",
    "X_train_rnn_normalized3 =normalize_data(X_train_rnn3_re)\n",
    "\n",
    "# reshape to 3D for RNN input\n",
    "X_train_rnn_normalized3 = X_train_rnn_normalized3.reshape((X_train_rnn3.shape[0]*X_train_rnn3.shape[1],n_steps,xdim2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdim2\n",
    "# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_n_units(n_units=12):\n",
    "    m = tf.keras.metrics.RootMeanSquaredError()\n",
    "    drop_rate=0.1\n",
    "    # create model\n",
    "    bigru_model = tf.keras.Sequential()\n",
    "    bigru_model.add(tf.keras.layers.Input(shape=(12,63)))\n",
    "    bigru_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True)))\n",
    "    bigru_model.add(tf.keras.layers.BatchNormalization())\n",
    "    bigru_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True)))\n",
    "    bigru_model.add(tf.keras.layers.BatchNormalization())\n",
    "    bigru_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(n_units)))\n",
    "    bigru_model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "    bigru_model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "    bigru_model.add(tf.keras.layers.Dense(12,activation='linear'))\n",
    "    \n",
    "    # compile model\n",
    "    bigru_model.compile(optimizer='adam', loss=\"mean_squared_error\", \n",
    "                        metrics=m)\n",
    "    \n",
    "    return bigru_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = KerasClassifier(build_fn=create_model_n_units, epochs=20,\n",
    "                         batch_size=256,verbose=0)\n",
    "\n",
    "n_units= [10, 25, 50, 75, 100]\n",
    "param_grid1 = dict(n_units=n_units)\n",
    "\n",
    "grid = GridSearchCV(estimator=model1, param_grid=param_grid1, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_rnn_normalized3, y_train_rnn3_re)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_units = 50\n",
    "drop_rate = 0.2\n",
    "# dim_embed = 50\n",
    "# Define learning rate scheduler\n",
    "lrate = LearningRateScheduler(scheduler)\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "loss = \"mean_squared_error\"\n",
    "metrics = ['accuracy', lr_metric]\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "verbose = 1\n",
    "\n",
    "# define model\n",
    "bigru_model = tf.keras.Sequential()\n",
    "bigru_model.add(tf.keras.layers.Input(shape=(12,xdim2)))\n",
    "bigru_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True)))\n",
    "bigru_model.add(tf.keras.layers.BatchNormalization())\n",
    "# bigru_model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "bigru_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True)))\n",
    "bigru_model.add(tf.keras.layers.BatchNormalization())\n",
    "bigru_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(n_units, return_sequences=True)))\n",
    "bigru_model.add(tf.keras.layers.BatchNormalization())\n",
    "bigru_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(n_units)))\n",
    "bigru_model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "# gru_model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "# bigru_model.add(tf.keras.layers.Dense(8,activation='relu'))\n",
    "bigru_model.add(tf.keras.layers.Dense(12,activation='linear'))\n",
    "# lstm_model.add(tf.keras.layers.Activation('softmax'))\n",
    "# Compile model\n",
    "\n",
    "# Fit\n",
    "bigru_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "bigru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigru_history = bigru_model.fit(X_train_rnn_normalized1, \n",
    "#                                 y_train_rnn1_re, batch_size=batch_size, epochs=epochs, \n",
    "#                                 validation_split=validation_split, \n",
    "#                                 callbacks=callbacks_list,verbose=verbose)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
