{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from dask import dataframe as dd\n",
    "# from icd9cms.icd9 import search\n",
    "import re\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medicare summary file with exposure and census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/projects/n/nsaph_common/conda/envs/nsaph_tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nfs/projects/n/nsaph_common/conda/envs/nsaph_tensorflow/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# read in using dask\n",
    "# df = dd.read_csv('/nfs/nsaph_ci3/ci3_analysis/mortality_prediction/medicare_2011_2013.csv')\n",
    "# med_df = pd.read_csv('/nfs/nsaph_ci3/ci3_analysis/mortality_prediction/medicare_2011_2013.csv')\n",
    "# med_df = pd.read_csv('/nfs/nsaph_ci3/ci3_analysis/mortality_prediction/medicare_2011_2016.csv')\n",
    "\n",
    "c_size = 100000\n",
    "reader = pd.read_csv('medicare_2011_2016.csv', chunksize=c_size)\n",
    "df_ls = []\n",
    "\n",
    "for chunk in reader:\n",
    "    # read in chunk at a time\n",
    "    c = chunk.drop(columns=['Unnamed: 0','latitude', 'longitude', 'zcta', 'qid', 'tmmx',\n",
    "                     'ozone', 'pm25_nn', 'pm25_ensemble'])\n",
    "    \n",
    "    c_sub = c.dropna(subset=['zip'])\n",
    "    df_ls.append(c_sub)\n",
    "        \n",
    "med_df = pd.concat(df_ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why missing Nan values for census data?\n",
    "- covariates merged by zip and year\n",
    "- health data (medicare) merged by zip\n",
    "So, it looks like there's missing info for that zipcode & year.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df3 = med_df.dropna()\n",
    "# med_df2 = df.dropna().compute()\n",
    "print(med_df3.shape, med_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by state and zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/tools/lib/anaconda/3-5.2.0/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def map_sex_binary(val):\n",
    "    if val == 1: # male\n",
    "        ans = 0\n",
    "    else: # female\n",
    "        ans = 1\n",
    "    return ans\n",
    "\n",
    "med_df3['sex'] = med_df3['sex'].apply(map_sex_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_binary(med_df2):\n",
    "    # convert race to binary columns\n",
    "    df = pd.get_dummies(med_df2['race'])\n",
    "    \n",
    "    df.rename(columns={0:'race_0', 1:'race_1', 2:'race_2', 3:'race_3', 4:'race_4', 5:'race_5', 6:'race_6'}, inplace=True)\n",
    "    \n",
    "    med_concat = pd.concat([med_df2,df],axis=1)\n",
    "    \n",
    "    med_concat2 = med_concat.drop(columns=['race'])\n",
    "    \n",
    "    return med_concat2\n",
    "\n",
    "med_df4 = race_binary(med_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((229150881, 27), (229150881, 21))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_df4.shape, med_df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zip', 'year', 'sex', 'age', 'statecode', 'dual', 'death', 'dead',\n",
       "       'poverty', 'popdensity', 'medianhousevalue', 'pct_blk',\n",
       "       'medhouseholdincome', 'pct_owner_occ', 'hispanic', 'education',\n",
       "       'smoke_rate', 'mean_bmi', 'rmax', 'pr', 'race_0', 'race_1', 'race_2',\n",
       "       'race_3', 'race_4', 'race_5', 'race_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_agg = {'sex':'mean', 'age':'mean', 'dual':'sum', 'death':'sum',\n",
    "       'dead':'mean',  \n",
    "       'poverty':'mean', 'popdensity':'mean','medianhousevalue':'mean', \n",
    "       'pct_blk':'mean', 'medhouseholdincome':'mean', 'pct_owner_occ':'mean',\n",
    "       'hispanic':'mean', 'education':'mean', 'smoke_rate':'mean', \n",
    "       'mean_bmi':'mean','rmax':'mean', 'pr':'mean',\n",
    "       'race_0':'mean', 'race_1':'mean', 'race_2':'mean', 'race_3':'mean', \n",
    "       'race_4':'mean', 'race_5':'mean', 'race_6':'mean'}\n",
    "\n",
    "zip_agg = med_df4.groupby(['statecode','zip','year']).aggregate(col_to_agg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2011, 2012, 2013, 2014, 2015, 2016])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_agg = zip_agg.reset_index()\n",
    "np.unique(zip_agg.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_agg.to_csv(\"medicare_aggregated_zip_2011_2012.csv\")\n",
    "zip_agg.to_csv(\"medicare_aggregated_zip_2011_2016.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medicare Hospitalization Admission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df = pd.read_csv('/nfs/nsaph_ci3/ci3_analysis/mortality_prediction/medicare_hosp_admin_2011_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df.dropna(inplace=True)\n",
    "hosp_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convert icd9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neoplasms(hosp_df, diag_col):\n",
    "    # all fall under category neoplasms (140 - 239)\n",
    "    codes = {'140-149': [], '150-159': [], '160-165': [],\n",
    "        '170-176': [], '179-189':[], '190-199':[], '200-209':[],\n",
    "        '210-229':[], '230-234': [], '235-238': [], '239': []}\n",
    "\n",
    "    for i in range(hosp_df.shape[0]): #hosp_df.shape[0]\n",
    "        \n",
    "        # get overarching category\n",
    "        c = search(hosp_df.loc[i,diag_col])\n",
    "        while True:\n",
    "            if c == None: # not in icd9cms dictionary\n",
    "                break\n",
    "            elif c.parent == None: # found overarching category of neoplasms\n",
    "                break\n",
    "            else: # keep searching through tree       \n",
    "                c = c.parent\n",
    "                \n",
    "        # first check if in overarching neoplasms category (if parent is neoplasm)\n",
    "        if c!= None and re.findall('140|239',str(c)):\n",
    "            code = str(search(hosp_df.loc[i,diag_col]))\n",
    "            if re.findall('14[0-9]',code):\n",
    "                codes['140-149'].append(i)\n",
    "            if re.findall('15[0-9]',code):\n",
    "                codes['150-159'].append(i)\n",
    "            if re.findall('16[0-5]',code):\n",
    "                codes['160-165'].append(i)\n",
    "            if re.findall('17[0-6]',code):\n",
    "                codes['170-176'].append(i)\n",
    "            if re.findall('179|18[0-9]',code):\n",
    "                codes['179-189'].append(i)\n",
    "            if re.findall('19[0-9]',code):\n",
    "                codes['190-199'].append(i)\n",
    "            if re.findall('20[0-9]',code):\n",
    "                codes['200-209'].append(i)\n",
    "            if re.findall('21[0-9]|22[0-9]',code):\n",
    "                codes['210-229'].append(i)\n",
    "            if re.findall('23[0-4]',code):\n",
    "                codes['230-234'].append(i)\n",
    "            if re.findall('23[5-8]',code):\n",
    "                codes['235-238'].append(i)\n",
    "            if re.findall('239',code):\n",
    "                codes['239'].append(i)\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find neoplasms in principle diagnosis\n",
    "c = find_neoplasms(hosp_df, 'DIAG1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find neoplasms in secondary diagnosis\n",
    "c2 = find_neoplasms(hosp_df, 'DIAG2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize neoplasm diagnosis binary indicators - dont run again\n",
    "hosp_df['neo_140_149'] = 0\n",
    "hosp_df['neo_150_159'] = 0\n",
    "hosp_df['neo_160_165'] = 0\n",
    "hosp_df['neo_170_176'] = 0\n",
    "hosp_df['neo_179_189'] = 0\n",
    "hosp_df['neo_190_199'] = 0\n",
    "hosp_df['neo_200_209'] = 0\n",
    "hosp_df['neo_210_229'] = 0\n",
    "hosp_df['neo_230_234'] = 0\n",
    "hosp_df['neo_235_238'] = 0\n",
    "hosp_df['neo_239'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDiag(df,c):\n",
    "    hosp_df2 = df.copy()\n",
    "    \n",
    "    for key in c.keys():\n",
    "        r = c[key]\n",
    "\n",
    "        if key == '140-149':\n",
    "            hosp_df2.loc[r,'neo_140_149'] = 1\n",
    "            \n",
    "        if key == '150-159':\n",
    "            hosp_df2.loc[r,'neo_150_159'] = 1\n",
    "            \n",
    "        if key == '160-165':\n",
    "            hosp_df2.loc[r,'neo_160_165'] = 1\n",
    "            \n",
    "        if key == '170-176':\n",
    "            hosp_df2.loc[r,'neo_170_176'] = 1\n",
    "            \n",
    "        if key == '179-189':\n",
    "            hosp_df2.loc[r,'neo_179_189'] = 1\n",
    "            \n",
    "        if key == '190-199':\n",
    "            hosp_df2.loc[r,'neo_190_199'] = 1\n",
    "            \n",
    "        if key == '200-209':\n",
    "            hosp_df2.loc[r,'neo_200_209'] = 1\n",
    "            \n",
    "        if key == '210-229':\n",
    "            hosp_df2.loc[r,'neo_210_229'] = 1\n",
    "            \n",
    "        if key == '230-234':\n",
    "            hosp_df2.loc[r,'neo_230_234'] = 1\n",
    "            \n",
    "        if key == '235-238':\n",
    "            hosp_df2.loc[r,'neo_235_238'] = 1\n",
    "            \n",
    "        if key == '239':\n",
    "            hosp_df2.loc[r,'neo_239'] = 1\n",
    "            \n",
    "    return hosp_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dictionary id's together fro diag1 and diag2\n",
    "c3 = {'140-149': [], '150-159': [], '160-165': [],\n",
    "        '170-176': [], '179-189':[], '190-199':[], '200-209':[],\n",
    "        '210-229':[], '230-234': [], '235-238': [], '239': []}\n",
    "\n",
    "for k in c3.keys():\n",
    "    c3[k] = set(c[k] + c2[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add binary indicator of principle or secondary diagnosis code\n",
    "hosp_df_diag = addDiag(hosp_df, c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse zipcode, add month and year, aggregate by ZIP, month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily drop columns not using (may use later)\n",
    "sub_hosp_df = hosp_df_diag.drop(columns=['Unnamed: 0','ADM_TYPE','DIAG1','DIAG2','DIAG3'])\n",
    "\n",
    "def reverse_zip(val):\n",
    "    z = int(str(int(val))[::-1])\n",
    "    return z\n",
    "\n",
    "# get zipcode from reversed zipcodes\n",
    "sub_hosp_df['zip'] = sub_hosp_df['zipcode_R'].apply(reverse_zip)\n",
    "\n",
    "# drop zipcode_r\n",
    "sub_hosp_df.drop(columns=['zipcode_R'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def admin_yr(val):\n",
    "    a = int(val[-4:])\n",
    "    return a\n",
    "\n",
    "def admin_month(val):\n",
    "    a = val[2:-4]\n",
    "    return a\n",
    "\n",
    "\n",
    "# get admission year from admission date (because year is actually referring to discharge date)\n",
    "sub_hosp_df['AYEAR'] = sub_hosp_df['ADATE'].apply(admin_yr)\n",
    "\n",
    "sub_hosp_df['AMONTH'] = sub_hosp_df['ADATE'].apply(admin_month)\n",
    "\n",
    "# drop columns not necessary for aggregating\n",
    "# year - discharge date, not admission date year\n",
    "sub_hosp_df.drop(columns=['QID','ADATE', 'YEAR', 'index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_agg2 = {'ICU_DAY': sum, 'CCI_DAY': sum, 'LOS':'mean',\n",
    "       'Parkinson_pdx2dx_25':sum, 'Alzheimer_pdx2dx_25':sum, 'Dementia_pdx2dx_25':sum,\n",
    "       'CHF_pdx2dx_25':sum, 'AMI_pdx2dx_25':sum, 'COPD_pdx2dx_25':sum, 'DM_pdx2dx_25':sum,\n",
    "       'Stroke_pdx2dx_25':sum, 'CVD_pdx2dx_25':sum, 'CSD_pdx2dx_25':sum,\n",
    "       'Ischemic_stroke_pdx2dx_25':sum, 'Hemo_Stroke_pdx2dx_25':sum,\n",
    "       'neo_140_149':sum, 'neo_150_159':sum, 'neo_160_165':sum,\n",
    "       'neo_170_176':sum, 'neo_179_189':sum, 'neo_190_199':sum, 'neo_200_209':sum,\n",
    "       'neo_210_229':sum, 'neo_230_234':sum, 'neo_235_238':sum, 'neo_239':sum}\n",
    "\n",
    "# groupby and aggregate by zipcode\n",
    "sub_hosp_df3 = sub_hosp_df.groupby(['zip', 'AYEAR', 'AMONTH']).aggregate(cols_to_agg2)\n",
    "\n",
    "# reset index\n",
    "sub_hosp_df3.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2170438, 29)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_hosp_df4 = sub_hosp_df3[sub_hosp_df3.AYEAR >=2011]\n",
    "sub_hosp_df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "sub_hosp_df4.to_csv('hosp_aggregated_zip_2011_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Medicare Monthly deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_size = 100000\n",
    "reader = pd.read_csv('medicare_pop_2011_2016.csv', chunksize=c_size)\n",
    "df_ls = []\n",
    "\n",
    "for chunk in reader:\n",
    "    # read in chunk at a time\n",
    "    \n",
    "    c = chunk.drop(columns=['Unnamed: 0'])\n",
    "    if c.population.isna().sum() != c_size:\n",
    "        c2 =c.dropna()\n",
    "        df_ls.append(c2)\n",
    "        \n",
    "merged_df = pd.concat(df_ls)\n",
    "\n",
    "agg_pop = merged_df.groupby(['zip','year']).mean()\n",
    "agg_pop = agg_pop.reset_index()\n",
    "\n",
    "agg_pop.to_csv('medicare_pop_aggregated_2011_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendix code\n",
    "cols_to_agg = {'ADATE': min, 'ICU_DAY': 'mean', 'CCI_DAY': 'mean', 'YEAR':min, 'LOS':'mean',\n",
    "       'Parkinson_pdx2dx_25':sum, 'Alzheimer_pdx2dx_25':sum, 'Dementia_pdx2dx_25':sum,\n",
    "       'CHF_pdx2dx_25':sum, 'AMI_pdx2dx_25':sum, 'COPD_pdx2dx_25':sum, 'DM_pdx2dx_25':sum,\n",
    "       'Stroke_pdx2dx_25':sum, 'CVD_pdx2dx_25':sum, 'CSD_pdx2dx_25':sum,\n",
    "       'Ischemic_stroke_pdx2dx_25':sum, 'Hemo_Stroke_pdx2dx_25':sum, 'zip':min}\n",
    "\n",
    "# groupby and aggregate by QID\n",
    "# sub_hosp_df2 = sub_hosp_df.groupby('QID').aggregate(cols_to_agg)\n",
    "\n",
    "def admin_yr(val):\n",
    "    a = int(val[-4:])\n",
    "    return a\n",
    "\n",
    "def admin_month(val):\n",
    "    a = val[:2]\n",
    "    return a\n",
    "    \n",
    "def admin_day(val):\n",
    "    a = val[2:-4]\n",
    "    return a\n",
    "\n",
    "# get admission year from admission date (because year is actually referring to discharge date)\n",
    "# sub_hosp_df2['AYEAR'] = sub_hosp_df2['ADATE'].apply(admin_yr)\n",
    "\n",
    "# get admission day of month\n",
    "sub_hosp_df2['ADAY'] = sub_hosp_df2['ADATE'].apply(admin_day)\n",
    "\n",
    "# get admission month\n",
    "sub_hosp_df2['AMONTH'] = sub_hosp_df2['ADATE'].apply(admin_month)\n",
    "\n",
    "# reset index\n",
    "sub_hosp_df2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_zips = pd.read_csv('../../ci3_confounders/data_for_analysis/prepped_census/zcta_tract_rel_10.txt')\n",
    "\n",
    "census_data = pd.read_csv('../../ci3_confounders/data_for_analysis/prepped_census/census_county_interpolated.csv')\n",
    "# census_data2 = pd.read_csv('../../ci3_confounders/data_for_analysis/prepped_census/census_county_uninterpolated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['ZCTA5', 'STATE', 'COUNTY', 'TRACT', 'GEOID', 'POPPT', 'HUPT', 'AREAPT',\n",
       "        ' AREALANDPT', 'ZPOP', 'ZHU', 'ZAREA', 'ZAREALAND', 'TRPOP', 'TRHU',\n",
       "        'TRAREA', 'TRAREALAND', 'ZPOPPCT', 'ZHUPCT', 'ZAREAPCT', 'ZAREALANDPCT',\n",
       "        'TRPOPPCT', 'TRHUPCT', 'TRAREAPCT', 'TRAREALANDPCT'],\n",
       "       dtype='object'),\n",
       " Index(['fips', 'NAME', 'land_area', 'year', 'hispanic_pct', 'poverty',\n",
       "        'poverty_mcare', 'population', 'median_house_value', 'blk_pct',\n",
       "        'white_pct', 'native_pct', 'asian_pct', 'no_grad', 'no_grad_mcare',\n",
       "        'median_household_income', 'owner_occupied', 'median_age',\n",
       "        'age_pct_0_14', 'age_pct_15_44', 'age_pct_45_65', 'age_pct_65_plus',\n",
       "        'population_density'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_zips.columns, census_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data2 = census_data.merge(census_zips[['ZCTA5','COUNTY','STATE']], how='left', left_on=['fips'], right_on=['COUNTY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64660, (64660, 26))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_data2.ZCTA5.isna().sum(), census_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]),\n",
       " array([3233, 3233, 3233, 3233, 3233, 3233, 3233, 3233]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(census_data.year,return_counts=True)\n",
    "sub_census = census_data[census_data['year']>=2011]\n",
    "np.unique(sub_census.year,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environmental Data, Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"../../ci3_confounders/data_for_analysis/earth_engine/temperature/temperature_daily_zipcode_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_seasonal= pd.read_csv(\"../../ci3_confounders/data_for_analysis/earth_engine/temperature/temperature_seasonal_zipcode_combined.csv\")\n",
    "temp_seasonal.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-5a2523a01b0e>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_temp['date'] = sub_temp['date'].apply(convert_datetime)\n"
     ]
    }
   ],
   "source": [
    "# subset to only include years after 2011\n",
    "sub_temp = temp[temp.year >=2011]\n",
    "\n",
    "def convert_datetime(val):\n",
    "    ans = pd.to_datetime(val, format='%Y-%m-%d')\n",
    "    return ans\n",
    "\n",
    "sub_temp['date'] = sub_temp['date'].apply(convert_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8e118fbca494>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_temp['month'] = sub_temp['date'].dt.month\n",
      "/nfs/projects/n/nsaph_common/conda/envs/nsaph_tensorflow/lib/python3.8/site-packages/pandas/core/frame.py:4305: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "<ipython-input-5-8e118fbca494>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_temp.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# get month, aggregate by year, month, zip\n",
    "sub_temp['month'] = sub_temp['date'].dt.month\n",
    "\n",
    "# drop columns not needed to aggregate\n",
    "sub_temp.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# drop nan values\n",
    "sub_temp.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate monthly\n",
    "temp_cols_agg = {'tmmx':'mean', 'rmax':'mean', 'pr':'mean'}\n",
    "temp_agg = sub_temp.groupby(['ZIP','year', 'month']).aggregate(temp_cols_agg)\n",
    "\n",
    "temp_agg.reset_index(inplace=True)\n",
    "\n",
    "# save results\n",
    "temp_agg.to_csv('monthly_temp_by_zip.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same with humidity, precipitation, pm, ozone, no2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = pd.read_csv(\"../../ci3_confounders/data_for_analysis/earth_engine/wind/wind_daily_zipcode_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89725136, 5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_sub = wind[wind['year'] >= 2011]\n",
    "wind_sub.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
